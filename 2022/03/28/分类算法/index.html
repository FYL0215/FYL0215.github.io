<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>机器学习之分类算法 | WELCOME！Tangerine’s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="决策树-随机森林机器学习中分类与回归的算法。决策树以树状结构表示数据分类的结果，叶子节点表示结果，非叶子节点表示测试函数。  优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。 缺点：可能会产生过度匹配（过拟合）的问题，可以通过剪枝进行处理 适用数据类型：数值型和标称型  使用过程：1.训练阶段：使用训练数据集构造决策树2.分类阶段：从根开始向下分类 一.基本概">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习之分类算法">
<meta property="og:url" content="https://github.com/FYL0215/2022/03/28/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/index.html">
<meta property="og:site_name" content="WELCOME！Tangerine’s Blog">
<meta property="og:description" content="决策树-随机森林机器学习中分类与回归的算法。决策树以树状结构表示数据分类的结果，叶子节点表示结果，非叶子节点表示测试函数。  优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。 缺点：可能会产生过度匹配（过拟合）的问题，可以通过剪枝进行处理 适用数据类型：数值型和标称型  使用过程：1.训练阶段：使用训练数据集构造决策树2.分类阶段：从根开始向下分类 一.基本概">
<meta property="og:locale">
<meta property="og:image" content="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292238822.png">
<meta property="og:image" content="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292238215.png">
<meta property="og:image" content="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292239033.png">
<meta property="og:image" content="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292239474.png">
<meta property="og:image" content="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292239144.png">
<meta property="og:image" content="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292239964.png">
<meta property="og:image" content="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292239102.png">
<meta property="og:image" content="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292239248.png">
<meta property="og:image" content="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292239834.png">
<meta property="og:image" content="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292239378.png">
<meta property="og:image" content="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292239904.png">
<meta property="og:image" content="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292239937.png">
<meta property="article:published_time" content="2022-03-28T12:54:44.000Z">
<meta property="article:modified_time" content="2022-03-29T15:03:32.120Z">
<meta property="article:author" content="FYL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292238822.png">
  
    <link rel="alternate" href="/FYL0215/atom.xml" title="WELCOME！Tangerine’s Blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/FYL0215/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/FYL0215/css/style.css">

  
    
<link rel="stylesheet" href="/FYL0215/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.1.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/FYL0215/" id="logo">WELCOME！Tangerine’s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/FYL0215/">Home</a>
        
          <a class="main-nav-link" href="/FYL0215/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/FYL0215/atom.xml" title="RSS"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Buscar"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Buscar"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://github.com/FYL0215"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-分类算法" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/FYL0215/2022/03/28/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/" class="article-date">
  <time class="dt-published" datetime="2022-03-28T12:54:44.000Z" itemprop="datePublished">2022-03-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      机器学习之分类算法
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="决策树-随机森林"><a href="#决策树-随机森林" class="headerlink" title="决策树-随机森林"></a>决策树-随机森林</h2><p>机器学习中分类与回归的算法。决策树以树状结构表示数据分类的结果，叶子节点表示结果，非叶子节点表示测试函数。</p>
<ul>
<li>优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。</li>
<li>缺点：可能会产生过度匹配（过拟合）的问题，可以通过剪枝进行处理</li>
<li>适用数据类型：数值型和标称型</li>
</ul>
<p><strong>使用过程：</strong><br>1.训练阶段：使用训练数据集构造决策树<br>2.分类阶段：从根开始向下分类</p>
<h3 id="一-基本概念："><a href="#一-基本概念：" class="headerlink" title="一.基本概念："></a>一.基本概念：</h3><p><strong>最大信息化增益：</strong>为了获得最大化信息增益需要进行节点划分，一般来说，子节点的不纯度越低，信息增益越大。然而，过度追求低子节点不纯度导致搜索空间急剧增大，因此大多数库中实现的是二叉决策树。</p>
<p><strong>不纯度衡量标准：</strong>基尼系数（IG），熵（IH），误分类率（IE）<br><em>熵</em>：当节点中所有特征属于同一个类别则熵为零;当样本以相同的比例分属不同的特征时熵的值最大<br><em>基尼系数</em>：降低误分类可能性的标准；同熵一样，当等比例分布时，基尼系数的值最大</p>
<!--熵与基尼系数产生的结果相似，不值用不纯度判断二叉决策树的好坏，应尝试不同的剪枝算法进行优化-->
<p><em>误分类率</em>：可以用于剪枝方法的标准，不能用于决策树的构建过程<!--对节点中样本数量不敏感--></p>
<p>三个标准在[0~1]之间的关系</p>
<p><img src="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292238822.png" alt="image-20220326204304629"></p>
<h3 id="二-构建过程："><a href="#二-构建过程：" class="headerlink" title="二.构建过程："></a>二.构建过程：</h3><p>决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类的过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。<br>1） 开始：构建根节点，将所有训练数据都放在根节点，选择一个最优特征(满足最大信息化增益)，按着这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。</p>
<p>2） 如果这些子集已经能够被基本正确分类，那么构建叶节点，并将这些子集分到所对应的叶节点去。</p>
<p>3）如果还有子集不能够被正确的分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的节点，如果递归进行，直至所有训练数据子集被基本正确的分类，或者没有合适的特征为止。</p>
<p>4）结束：每个子集都被分到叶节点上，即都有了明确的类，这样就生成了一颗决策树。</p>
<p>SK-learn利用熵作为不纯度度量标准构建决策树，注意：深度越大，决策边界越复杂越容易出现过拟合的现象</p>
<p>PS：export_graphviz导出决策树.dot文件，使用GraphViz可视化决策树</p>
<p><strong>SK-learn生成决策树：</strong><br><img src="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292238215.png" alt="image-20220326205446656"></p>
<hr>
<h3 id="随机森林：多颗决策树的集成"><a href="#随机森林：多颗决策树的集成" class="headerlink" title="随机森林：多颗决策树的集成"></a>随机森林：多颗决策树的集成</h3><p>集成：将若分类器（决策树）集成为强分类器（随机森林），其集成模型的鲁棒性更好</p>
<p>优势：不需要剪枝；更好的泛化误差；不易产生过拟合现象；不必担心超参值得选择</p>
<p>可以优化的超参：<br>决策树的数量：决策树的数量越多，随机森林整体的分类性能越好<br>bootstrap抽样的数量；<br>节点划分中使用特征的数量（随机森林节点划分时并不选择所有特征）</p>
<p>处理：通过选择bootstrap抽样中样本数量n，我们可以控制随机森林的偏差与方差权衡的问题。如果n的值较大，就降低了随机性，由此更可能导致随机森林的过拟合。反之，我们可以基于模型的性能，通过选择较小的n值来降低过拟合。包括scikit-learn中RandomForestClassifier在内的大多数对随机森林的实现中，bootstrap抽样的数量一般与原始训练集中样本的数量相同，因为这样在折中偏差与方差方面一般会有一个好的均街结果。而对于在每次节点划分中用到的特征<br>数量m，我们选择一个比训练集中特征总量小的值。在scikit-lcarn及其他程序实现中常用的默认值为d&#x3D;m^(0.5),m为训练中的样本总量</p>
<hr>
<h3 id="KNN算法"><a href="#KNN算法" class="headerlink" title="KNN算法"></a>KNN算法</h3><p>KNN算法是惰性学习算法，仅仅是对训练集数据的记忆功能，不会从中抽象出一个判别函数</p>
<p>机器学习算法可以划分为参数化模型和非参数化模型。当使用参数化模型时，需要我们通过训练数据估计参数，并通过学习得到一个模式，以便在无需原始训练数据信息的情况下对新的数据点进行分类操作。典型的参数化模型包括：感知器、逻辑斯谛回归和线性支持向量机等。与之相反，非参数化模型无法通过一组固定的参数来进行表征，而参数的数量也会随着训练数据的增加而递增，如决策树(包括随机森林)和SVM。</p>
<p><strong>KNN算法思路：</strong></p>
<p>1选择近邻的数量k和距离度量标准。</p>
<p>2)找到待分类样本的k个最近邻居。</p>
<p>3)根据最近邻的类标进行多数投票。</p>
<p>KNN本身不需要训练数据，只是依赖对训练集的记忆性进行分类。</p>
<p><strong>优点</strong>：可以快速适应新的训练数据；</p>
<p><strong>缺点</strong>：计算复杂度随样本数量的增多而呈线性增长；无需训练使得参数数量随样本数量的增加而增加，同时对于大型数据样本如何存储存在问题</p>
<p>常用的距离度量标准：欧几里得距离；曼哈顿距离；米科夫斯基距离</p>
<p><strong>关键点</strong>：</p>
<p>1）正确的K值是过拟合与欠拟合平衡的关键</p>
<p>2）选择适合样本实际的距离计算标准</p>
<p><strong>维度灾难</strong>：对于一个样本数量大小稳定的训练数据集，随着其特征数量的增加，样本中有具体值的特征数量变得极其稀疏（大多数特征的取值为空)。直观地说，可以认为即使是最近的邻居，它们在高维空间中的实际距离也是非常远的，因此难以给出一个合适的类标判定</p>
<p><strong>解决办法</strong>：正则化方法并不适用于诸如決策树和KNN等算法<!--适用于logisit回归-->，但可以使用特征选择和降维等技术来帮助其避免维度灾难</p>
<hr>
<p><strong>过拟合</strong>：模型很好的捕捉到了训练数据集中的模式，但是无法泛化到未知的新数据上</p>
<p>过拟合是机器学习中的常见问题，它是指模型在训练数据集上表现良好，但是用于未知数据(测试数据)时性能不佳。如果一个模型出现了过拟合问题，我们也说此模型有<strong>高方差（高随机性）</strong>，这有可能是因为使用了相关数据中过多的参数，从而使得模型变得过于复杂。同样，模型也可能面临欠拟合<strong>高偏差（高系统偏差）</strong>问题，这意味着模型过于简单，无法发现训练数据集中隐含的模式，这也会使得模型应用于未知数据时性能不佳。虽然我们目前只介绍了分类中的线性換型，但对于过拟合与次拟合问题，最好使用蛋加复杂的非线性决策边界来阐明，如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292239033.png" alt="image-20220326234427717"></p>
<p><strong>感知器分类算法的缺点：</strong>对于无法完美线性可分的样本，感知器算法将永远无法收敛，<strong>因此常使用线性分类器算法</strong></p>
<hr>
<p><strong>logistic regression</strong>:分类模型而不是回归模型</p>
<p>logistic回归可以用于二类别分类以及多类别分类（OVR技术），还能预测样本发生的几率</p>
<p><strong>几率比</strong>：下图中log值，p为正事件（所预测事件）的发生概率</p>
<p><img src="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292239474.png" alt="image-20220326224945573"></p>
<p>由于几率比取值范围是[0,1],因此可以将右式转化到所有实数的范围内，可以将对数几率转化为特征值与权重的线性表达式如下：</p>
<p><img src="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292239144.png" alt="image-20220326225006157"></p>
<p>从而上式中左式的输入即为我们的预测值</p>
<p>Logite函数的反函数叫做logistic函数或者sigmoid函数</p>
<p><img src="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292239964.png" alt="image-20220326230011809"></p>
<p><img src="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292239102.png" alt="image-20220326230018102"></p>
<p>sigmoid函数的特点：Z以实数值作为输入将映射到[0，1]区间，拐点位于Z&#x3D;0.5处</p>
<p><img src="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292239248.png" alt="image-20220326230233110"></p>
<p><strong>logistic回归与sigmoid函数的关系</strong>：logistic回归模型使用sigmiod函数作为激励函数，预测概率，获得量化器预测类标</p>
<p><strong>如果确定权重w？</strong>：通过梯度下降最小化代价函数实现所有权重的更新</p>
<p><strong>logistic过拟合处理</strong>：正则化是解决共线性的优秀方法，可以滤去数据中的噪声，从而防止过拟合。其背后是引入偏差（系统误差）对极端参数进行处罚。常用的正则化方法叫做L2正则化（L2收缩），或者权重衰减。要进行正则化，首先要进行特征缩放，确保所有特征的衡量标准保持统一。</p>
<p><img src="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292239834.png" alt="image-20220326233356963"></p>
<p>在代价函数中加入正则化项，增加拉姆他值就可以增加正则化强度，导致权重系数收缩</p>
<p>SK-learn中需要减小参数C的值增加正则化强度</p>
<hr>
<h3 id="SVM支持向量机"><a href="#SVM支持向量机" class="headerlink" title="SVM支持向量机"></a>SVM支持向量机</h3><p>使用支持向量机<strong>最大化分类间隔</strong><br>一种性能强大且广泛应用的学习算法是支持向量机(support vector machinc, SVM)，它可以看作对感知器的扩展。</p>
<p>在感知器算法中，我们可以<strong>最小化分类误差</strong>。</p>
<p>而在SVM中，我们的优化目标是<strong>最大化分类间隔</strong>。此处间隔是指两个分离的超平面(决策边界)间的距离，而最靠<br>近超平面的训练样本称作支持向量(support vector)，如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292239378.png" alt="image-20220326234855529"></p>
<p>&#x3D;&#x3D;<strong>logistic回归与SVM算法的异同</strong>：&#x3D;&#x3D;</p>
<p>两者往往能得到相似的结果，logistic回归更易于处理离群点，SVM算法更关注于接近决策边界的点；logistic模型更简单方便；</p>
<p>SVM用于解决非线性问题，logistic回归无法 用于非线性问题</p>
<p>LIBLIBEAR库或者LIBSVM实现SVM算法</p>
<p><strong>核方法SVM解决非线性问题：</strong>通过映射函数将样本的原始特征映射到一个使样本线性可分的更高维空间中。我们可以将二维数据集通过下列映射转换到新的三维特征空间中，从而使得样本可分。这使得我们可以将图中的两个类别通过线性超平面进行分割，如果我们把此超平面映射回原始特征空问，则可线性分割两类数据的超平面就交为非线性的了。</p>
<p><img src="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292239904.png" alt="image-20220327000542297"></p>
<p>常用的核函数：高斯核函数，“核”理解为一对样本之间的相似函数。高斯球面的截止参数y，y减小决策边界宽松；</p>
<p><img src="https://raw.githubusercontent.com/FYL0215/MyPicture/master/img/202203292239937.png" alt="image-20220327001359847"></p>
<p>在本章中，我们学到了许多不同的机器学习算法，可用于处理线性或者非线性问题。如果关注算法的可解释性，那么 决策树是一种特别具有吸引力的算法。逻辑斯谛回归不仅是可以通过梯度下降进行优化的一种有用的在线学习方法，而且可以给出待预测问题可能发生的概率。虽然支持向量机是一种强大的线性模型，而且可以通过核技巧扩展到非线性问题，但是为了达到好的预测效果，它需要调整众多的参数。而集成方法(如随机森林)不需要调整众多的参数且不像决策树那样容易产生过拟合现象，因此在解决实际问题中成为常用的一个模型作为惰性学习算法，k-近邻算法使得我们在分类领域可以尝试另外一种方式，它不是通过训练模型来进行预测，而更多的是通过计算来完成。但是，比选择合适的学习算法更重要的是：训练数据集中有什么样的可用数据。任何算法都无法使用缺乏翔实、无歧义的特征而获得好的预测结果。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/FYL0215/2022/03/28/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/" data-id="cl1cbmb2j0002lk9v9klx3eos" data-title="机器学习之分类算法" class="article-share-link">Compartir</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/FYL0215/2022/03/29/SpringBoot%E6%A1%86%E6%9E%B6/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Nuevo</strong>
      <div class="article-nav-title">
        
          SpringBoot框架
        
      </div>
    </a>
  
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archivos</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/FYL0215/archives/2022/03/">March 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Posts recientes</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/FYL0215/2022/03/29/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a>
          </li>
        
          <li>
            <a href="/FYL0215/2022/03/29/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a>
          </li>
        
          <li>
            <a href="/FYL0215/2022/03/29/SpringBoot%E6%A1%86%E6%9E%B6/">SpringBoot框架</a>
          </li>
        
          <li>
            <a href="/FYL0215/2022/03/28/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/">机器学习之分类算法</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2022 FYL<br>
      Construido por <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/FYL0215/" class="mobile-nav-link">Home</a>
  
    <a href="/FYL0215/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/FYL0215/js/jquery-3.4.1.min.js"></script>



  
<script src="/FYL0215/fancybox/jquery.fancybox.min.js"></script>




<script src="/FYL0215/js/script.js"></script>





  </div>
</body>
</html>